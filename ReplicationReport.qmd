---
title: "Replication of Intelligibility Task from Kutlu et al., The Impact of Race on Speech Perception and Accentedness Judgments in Racially Diverse and Non-Diverse Groups (2020, Applied Linguistics)"
author: "Erin Ye - erinye@stanford.edu"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

This study aimed to replicate the first task in Kutlu et al.'s 2022 study analyzing the impact of race on speech perception and accentedness judgment in racially diverse and non-diverse groups. Following the documented methods and using available materials from the original study, participants were recruited via Prolific to complete a one hour long experiment involving a language background questionnaire, a LexTALE language dominance task, and a social networks questionnaire prior to a speech intelligibility task. Three varieties of English (American, British, Indian) were paired in random order with either one of two Caucasian female faces or a South Asian female face. The study was motivated by the research question, how does race impact speech perception and social judgment of spoken English, and is there a difference in this impact depending on the racial diversity of one's social network?

The finding of interest from the original study was that participants reported lower intelligibility ratings for all varieties when speech was paired with South Asian faces. There was an additional finding that participants who reported less racially diverse social networks had higher accentedness judgment ratings than those with more racially diverse social networks; however, given the scope of the project and the length of the experiment task, I plan to focus on the first finding. 

## Methods

### Power Analysis

The original study found an effect size of t=10.9 with a group fo 58 participants. Based on the power analysis and the Cohen d-value of 2.913, the suggested sample size to achieve 95% power is 8 participants. Though that number seems small, the results of the power analysis depend on reported t-value, which in this case was very large. 

### Planned Sample

To ensure slightly more security, and to account for the need to recruit participants were sufficiently different social network diversity levels, I will aim for closer to 15 participants for my replication experiment. The participants in the original study were all American English dominant speakers. Speakers with 10 years or more of exposure to Hindi, Indian English, and British English will need to be excluded from the sample. The average age of participants was 19.2, and they were all college aged students, so I will aim for a similar demographic.

### Materials

"For visual stimuli, we used two previously normed and controlled face databases. South Asian faces were taken from the KKWETC face database (Satone 2017). White faces were taken from the Chicago face database (Ma et al. 2015). Both of these face databases have been used extensively in vision research. From each database, we picked three female faces that were shown to display no emotional valence (see Appendix A). Six white and South Asian faces (3:3 ratio) were matched with three different varieties by way of a fully randomized Latin-square counterbalance distribution (e.g., one white face with American English speaker 1, one white face with British English speaker 1, one white face with Indian English speaker 1, etc.). This yielded four lists for the intelligibility experiment. Each speaker was paired with one face per list and each participant only saw one list (n=120 sentences)." This is quoted directly from the original study and the faces were available on the OSF page.

For audio stimuli, the original recordings used in the Kutlu study were not available on the OSF page, but the same sentences from the HINT (Hidden Intent Coprus) lists were available via the ALLSTAR SpeechBox Corpus. I selected one Hindi L1 speaker, one American English speaker, and one British English speaker, whose recordings I then separated into sentence stimuli. "To make the task slightly challenging for the participants, and to assess whether noise impacts the intelligibility of different varieties of Englishes (Van Engen et al. 2014; Van Engen & Bradlow 2007), we added - 4 dB (signal to noise ratio) white noise to our recordings. This way, we also ensured that the task mimicked real-world scenarios where there is often background noise during speech perception and that participants paid more attention (i.e., listened to them carefully) to each stimulus. This was chosen based on previous studies (McGowan 2015) and also from the norming study."

### Procedure	

"Participants completed a language background questionnaire (Li et al. 2020), the LexTale English proficiency task (Lemhöfer & Broersma 2012), and the revised version of Lev-Ari’s (2017) social network questionnaire (see OSF page). They then completed the experimental
tasks, namely the intelligibility task followed by the accentedness judgment task. All testing was completed in a quiet room. The experiment took 1.5 hours to complete, and participants received 3 class credits upon completion of the experiment. The accentedness judgment and the intelligibility tasks were administered via PsychoPy (Peirce 2017). First, an image was shown on the computer screen for 250 ms. Then, the auditory stimulus played. The PsychoPy script was written in such a way that participants were not allowed to start typing nor judge the sentences in either experiment before the stimulus played fully. Participants were instructed regarding this information and completed a practice trial with the research assistant that consisted of three sentences which were excluded from the analysis. For the intelligibility task, participants were instructed to look at a speaker’s face on the screen, listen to the speech in noise carefully, and type the sentences as accurately as possible.
To facilitate typing, participants were advised to avoid punctuation or capital letters."

I will be following this procedure closely, with the exception that my experiment only includes the intelligibility task and will be run in JsPsych, not PsychoPy. It will also be administered completely online through Prolific. Participants will be rewarded monetarily rather than through class credit. They will also be debriefed at the end of the study.

### Analysis Plan

"Depending on participants’ answers to the social network questionnaire, those who reported having more than one tie with someone in their close network (i.e., someone that they interact with almost every day) who identifies with a racial or ethnic background outside of their own
racial and ethnic background were grouped in the MoreDiverse category. Those with ties only to individuals within their own racial and ethnic background were grouped in the LessDiverse."  

"Intelligibility scores were calculated through transcription accuracy. For each sentence, content words were selected and their transcription accuracy was calculated by means of 1s for correct content words and 0s for incorrect words (Porretta et al. 2016). Typos that were close to the content words were counted as 1s (e.g., yelow instead of yellow). To investigate proportions as a function of different race, English variety, and different social network diversity, we constructed a linear mixed-effects model using the lme4 package (Bates et al. 2018) in R (R version 3.6.1; R Core Team, 2019), conducted follow-up tests with lsmeans package (Lenth 2016), and corrected pairwise comparisons with the Bonferroni correction. Proportions were entered as the continuous dependent variable. As fixed effects, we included (1) Helmert-coded Variety (American English vs. British English), and (Indian English vs American + British English), (2) treatment coded Face (South Asian (a), white (b)), (3) treatment coded Diverse (LessDiverse (a), MoreDiverse(b)). Random effects were by-subject and by-item random intercepts. Random slopes were eliminated as the model did not converge." Participants who fail to transcribe more than 50% of the sentence stimuli will be excluded from the analysis."

**Clarify key analysis of interest here**  
The key analysis will be a mixed-effects model measuring comparisons of transcription accuracy for South Asian faces compared to white faces. At this point I have not planned any additional analyses beyond the methods described in the study.

### Differences from Original Study

While the goal is to minimize differences from the original study, mine will primarily differ in that different speakers were chosen for the auditory stimuli, since the original speaker recordings were not available. Additionally, the original experiment was administered in person on a college campus, whereas my experiment will be run completely online. My sample was also significantly smaller than the original study's based on our power analysis.

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
The sample included 14 users from Prolific; originally, 15 samples were collected, but one was excluded for insufficient completion of the task. Participants were sourced from the UK and the US, and I excluded users who spoke Hindi. The average age was 35.6.

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.
In my pre-data collection plans, I had mentioned excluding participants with extensive exposure to Indian English and British English, and had also aimed for participants from a similar age range to the original study. However, I ended up not screening for age. This admittedly was not done on purpose — since the intelligibility score finding was did not mention age as a significant finding, I did not remember to screen Prolific participants for these factors. Hopefully this allows us to analyze the generalizability of Kutlu's findings, although any resulting errors I take full responsibility for.


## Results


### Data preparation

Data preparation following the analysis plan.
	
```{r include=F}
### Data Preparation
#### Load Relevant Libraries and Functions
library(stringr)
library (stringdist)
library(dplyr)
library (readr)
library("knitr")
library("GGally")
library("janitor")
library("tidyverse")
library(lme4)
library(ggplot2)
#### Import data
df <- read_csv("ling245b_data.csv")

calculate_similarity <- function(actual, response, max_dist = 1) {
  # Clean and split into words
  clean_text <- function(text) {
    text %>%
      tolower() %>%
      str_remove_all("[^a-zA-Z ]") %>%  # Keep letters and spaces
      str_squish() %>%
      str_split(" +") %>%
      unlist()
  }
  
  actual_words <- clean_text(actual)
  response_words <- clean_text(response)
  
  if(length(actual_words) == 0) return(NA_real_)
  
  # Track matches while avoiding double-counting
  matches <- 0
  remaining_response <- response_words
 
    for(word in actual_words) {
    if(length(remaining_response) == 0) break
    
    distances <- stringdist::stringdist(word, remaining_response, method = "lv")
    
    # Handle empty distances and NA values
    if(length(distances) == 0 || all(is.na(distances))) next
    
    min_dist <- min(distances, na.rm = TRUE)
    
    # Add validity check for min_dist
    if(!is.na(min_dist) && min_dist <= max_dist) {
      matches <- matches + 1
      remaining_response <- remaining_response[-which.min(distances)]
    }
  }
   # Return both strict accuracy and similarity percentage
  list(
    accuracy = matches / length(actual_words),
    similarity = matches / max(length(actual_words), length(response_words))
  )
  }
  

# Apply to dataframe
df <- df %>%
  mutate(
    actual = as.character(actual_sentence),
    response = as.character(subject_response)
  ) %>%
  rowwise() %>%
  mutate(
    accuracy = calculate_similarity(actual, response)$accuracy
  ) %>%
  ungroup()
df$subject_response_unfixed <- NULL

df <- df %>% mutate(df = sub("_.*", "", stimulus_face)) %>% unite(df, stimulus_voice, col=face_voice, sep = " ", remove=FALSE) %>% select(-df)

head(df)
```

### Confirmatory analysis

```{r include=FALSE, eval=TRUE}

theme_set(theme_classic())

df$stimulus_face <- relevel(factor(df$stimulus_face), ref="south asian")

m1 <- lmer(accuracy ~ diverse * stimulus_voice * stimulus_face + (1 | subject_id) + (1 | stimulus) + (1 | `face shown`), 
           REML = F,
           data=df)

summary(m1)

```

### Additional analysis
``` {r, include=FALSE, eval=TRUE}
simple <- lmer(accuracy ~ stimulus_voice + (1 | subject_id), REML = F,
           data=df)
complex_1 <- lmer(accuracy ~ stimulus_voice * diverse + (1 | subject_id) + (1 | stimulus) + (1 | `face shown`), REML = F,
           data=df)
complex_2 <- lmer(accuracy ~ stimulus_voice * stimulus_face + (1 | subject_id) + (1 | stimulus) + (1 | `face shown`), REML = F,
           data=df)

#likelihood ratio testing by complexity of model
anova(simple, complex_1)
anova(complex_1, complex_2)
anova(complex_2, m2)

```

*Comparison with original graph*

![Original Figure](original_figure.png){width=2000}

```{r include=FALSE, eval=TRUE}
plot <- ggplot(data=df, aes(x=face_voice, y= accuracy, fill=face_voice)) + geom_violin(trim = FALSE, alpha = 0.5) +
  geom_boxplot(width = 0.2, outlier.shape = NA, color = "white") +
  geom_jitter(width = 0.3, size = 1, alpha = 0.2, color = "white") +
  facet_wrap(~diverse, labeller = as_labeller(
    c("less" = "Racially Less Diverse", 
      "more" = "Racially More Diverse"))) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  ) +
  labs(x = NULL, y = "proportion") + scale_y_continuous(breaks = seq(0, 1.0, by = .1))

show(plot)
  

```
```{r, include=FALSE, eval=TRUE}
# Looking at proportions ungrouped by social network diversity to get a better sense of numbers
 p_ungrouped <- ggplot(data=df, aes(x=face_voice, y= accuracy, fill=face_voice)) + geom_violin(trim = FALSE, alpha = 0.5) +
  geom_boxplot(width = 0.2, outlier.shape = NA, color = "white") +
  geom_jitter(width = 0.3, size = 1, alpha = 0.2, color = "white") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  ) +
  labs(x = NULL, y = "proportion") + scale_y_continuous(breaks = seq(0, 1.0, by = .1)) + ggtitle("Proportions Ungrouped by Network Diversity")

show(p_ungrouped)
```
## Discussion

### Summary of Replication Attempt

The present study aimed to replicate the intelligibility task portion of the Kutlu et al. (2022) study, which found that regardless of the diversity of one's social network that South Asian faces resulted in less transcription accuracy across varieties of English. Ultimately, our findings fail to replicate those produced in the original study. Running a linear mixed effects model accounting for individual subjects as a random effect, we find no statistically significant differences in transcription accuracy regardless of social network diversity, the race of the faces shown, or the variety of English participants heard.

### Commentary
In addition to the linear mixed effects model used by Kutlu et al., I also explored likelihood ratio comparisons of a simple model that only accounts for stimulus voice (variety of English), a model that accounts for voice and face shown, and the full complex model including voice, face, and diversity of social network. After running the ANOVA comparison of the simple model and the intermediate model, there was a significant difference in fixed effects, whereas the ANOVA comparing the intermediate model to the full complex model had no significant difference. From this analysis, we concur that there were no significant differences in transcription accuracy based upon which face was shown to the participant, which fails to replicate the original finding. 

As mentioned in the analysis plan, there are a couple of key differences in our replication study compared to the original study: ours was administered online, and with an older participant pool. We also used different stimuli from the SpeechBox corpus, and it is possible that our manipulation of adding 4dB noise was not exactly what the authors had intended, since we did not have a reference stimulus to compare ours to.

We may also consider the fact that since the original reported a high-powered t-value, our power analysis reported that a significantly smaller sample was needed to replicate its results. This may be a case of winner's curse — perhaps we would have observed significant effects with a larger sample, but the original study got so lucky with effects that the resulting power analysis ended up being too weak. That being said, the present study had 14 participants, six more than the suggested eight from the power analysis.

In terms of the analysis itself, a question I have regarding the original study pertains to the categorization of social network diversity into either "more" or "less." Perhaps a more descriptive analysis would have had more fine-grained categorical variables, or illustrated the network as continuous compared to discrete/binary. It may be important to consider whether the experimental design of the study, and its corresponding analysis, align with the research question of how social network diversity impact speech perception and intelligibility. Further research may be necessary to draw conclusions on the results put forth by Kutlu et al., although the current study hopes to prompt deeper inquiry in the field of speech perception and psycholinguistics.
