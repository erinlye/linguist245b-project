### Data Preparation
#### Load Relevant Libraries and Functions
install.packages("stringdist")
library (stringdist)
#### Import data
#### Data exclusion / filtering
#### Prepare data for analysis - create columns etc.
### Data Preparation
#### Load Relevant Libraries and Functions
install.packages("stringdist")
install.packages("readr")
library (stringdist)
library (readr)
#### Import data
df <- read_csv("linguist245b_data")
#### Import data
df <- read_csv("linguist245b_data.csv")
#### Import data
df <- read_csv("ling245b_data.csv")
install.packages("dplyr")
install.packages("udpipe")
library(dplyr)
library(udpipe)
model <- udpipe_download_model("english-ewt")
model <- udpipe_load_model(model$file_model)
# Function to extract content words with POS filtering
get_content_words <- function(text) {
annotated <- udpipe_annotate(model, text)
as.data.frame(annotated) %>%
filter(upos %in% c("NOUN", "VERB", "ADJ", "ADV")) %>%
pull(token) %>%
tolower()
}
# Calculate accuracy with typo tolerance
calculate_accuracy <- function(actual, response) {
actual_words <- get_content_words(actual)
response_words <- get_content_words(response)
if(length(actual_words) == 0) return(NA)
matches <- 0
remaining_response <- response_words
for(word in actual_words) {
distances <- stringdist(word, remaining_response, method = "lv")
if(length(distances) > 0 && min(distances) <= 1) {
matches <- matches + 1
remaining_response <- remaining_response[-which.min(distances)]
}
}
matches / length(actual_words)
}
#### Prepare data for analysis - create columns etc.
# Apply to dataframe
df <- df %>%
rowwise() %>%
mutate(accuracy = calculate_accuracy(actual_sentence, subject_response)) %>%
ungroup()
```
#### Data exclusion / filtering
model <- udpipe_download_model("english-ewt")
model <- udpipe_load_model(model$file_model)
get_content_words <- function(text) {
# Handle NA and empty strings
if(is.na(text) || str_trim(text) == "") return(character(0))
# Convert to character if not already
text <- as.character(text)
# Annotate with error handling
annotated <- tryCatch(
udpipe_annotate(model, text),
error = function(e) return(NULL)
)
if(is.null(annotated)) return(character(0))
as.data.frame(annotated) %>%
filter(upos %in% c("NOUN", "VERB", "ADJ", "ADV")) %>%
pull(token) %>%
tolower() %>%
str_remove_all("[^a-zA-Z]")  # Remove non-alphabetic characters
}
calculate_accuracy <- function(actual, response) {
# Convert to character if not already
actual <- as.character(actual)
response <- as.character(response)
actual_words <- get_content_words(actual)
response_words <- get_content_words(response)
if(length(actual_words) == 0) return(NA_real_)
matches <- 0
remaining_response <- response_words
for(word in actual_words) {
if(length(remaining_response) == 0) break
# Find closest match with max 1 Levenshtein distance
distances <- stringdist(word, remaining_response, method = "lv")
min_dist <- min(distances)
if(min_dist <= 1) {
matches <- matches + 1
# Remove matched word from response pool
remaining_response <- remaining_response[-which.min(distances)]
}
}
matches / length(actual_words)
}
#### Prepare data for analysis - create columns etc.
df <- df %>%
mutate(
actual_sentence = as.character(actual_sentence),
subject_response = as.character(subject_response)
) %>%
rowwise() %>%
mutate(
accuracy = tryCatch(
calculate_accuracy(actual_sentence, subject_response),
error = function(e) NA_real_
)
) %>%
ungroup()
head(df)
#### Import data
df <- read_csv("ling245b_data.csv")
calculate_similarity <- function(actual, response, max_dist = 1) {
# Clean and split into words
clean_text <- function(text) {
text %>%
tolower() %>%
str_remove_all("[^a-zA-Z ]") %>%  # Keep letters and spaces
str_squish() %>%
str_split(" +") %>%
unlist()
}
actual_words <- clean_text(actual)
response_words <- clean_text(response)
if(length(actual_words) == 0) return(NA_real_)
# Track matches while avoiding double-counting
matches <- 0
remaining_response <- response_words
for(word in actual_words) {
if(length(remaining_response) == 0) break
distances <- stringdist(word, remaining_response, method = "lv")
min_dist <- min(distances)
if(min_dist <= max_dist) {
matches <- matches + 1
remaining_response <- remaining_response[-which.min(distances)]
}
}
# Return both strict accuracy and similarity percentage
list(
accuracy = matches / length(actual_words),
similarity = matches / max(length(actual_words), length(response_words))
)
}
# Apply to dataframe
df <- df %>%
mutate(
actual = as.character(actual_sentence),
response = as.character(subject_response)
) %>%
rowwise() %>%
mutate(
accuracy = calculate_similarity(actual, response)$accuracy
) %>%
ungroup()
install.packages("stringr")
library(stringr)
calculate_similarity <- function(actual, response, max_dist = 1) {
# Clean and split into words
clean_text <- function(text) {
text %>%
tolower() %>%
str_remove_all("[^a-zA-Z ]") %>%  # Keep letters and spaces
str_squish() %>%
str_split(" +") %>%
unlist()
}
actual_words <- clean_text(actual)
response_words <- clean_text(response)
if(length(actual_words) == 0) return(NA_real_)
# Track matches while avoiding double-counting
matches <- 0
remaining_response <- response_words
for(word in actual_words) {
if(length(remaining_response) == 0) break
distances <- stringdist(word, remaining_response, method = "lv")
min_dist <- min(distances)
if(min_dist <= max_dist) {
matches <- matches + 1
remaining_response <- remaining_response[-which.min(distances)]
}
}
# Return both strict accuracy and similarity percentage
list(
accuracy = matches / length(actual_words),
similarity = matches / max(length(actual_words), length(response_words))
)
}
# Apply to dataframe
df <- df %>%
mutate(
actual = as.character(actual_sentence),
response = as.character(subject_response)
) %>%
rowwise() %>%
mutate(
accuracy = calculate_similarity(actual, response)$accuracy
) %>%
ungroup()
calculate_similarity <- function(actual, response, max_dist = 1) {
# Clean and split into words
clean_text <- function(text) {
text %>%
tolower() %>%
str_remove_all("[^a-zA-Z ]") %>%  # Keep letters and spaces
str_squish() %>%
str_split(" +") %>%
unlist()
}
for(word in actual_words) {
if(length(remaining_response) == 0) break
distances <- stringdist::stringdist(word, remaining_response, method = "lv")
# Handle empty distances and NA values
if(length(distances) == 0 || all(is.na(distances))) next
min_dist <- min(distances, na.rm = TRUE)
# Add validity check for min_dist
if(!is.na(min_dist) && min_dist <= max_dist) {
matches <- matches + 1
remaining_response <- remaining_response[-which.min(distances)]
}
}
actual_words <- clean_text(actual)
response_words <- clean_text(response)
if(length(actual_words) == 0) return(NA_real_)
# Track matches while avoiding double-counting
matches <- 0
remaining_response <- response_words
for(word in actual_words) {
if(length(remaining_response) == 0) break
distances <- stringdist(word, remaining_response, method = "lv")
min_dist <- min(distances)
if(min_dist <= max_dist) {
matches <- matches + 1
remaining_response <- remaining_response[-which.min(distances)]
}
}
# Return both strict accuracy and similarity percentage
list(
accuracy = matches / length(actual_words),
similarity = matches / max(length(actual_words), length(response_words))
)
}
# Apply to dataframe
df <- df %>%
mutate(
actual = as.character(actual_sentence),
response = as.character(subject_response)
) %>%
rowwise() %>%
mutate(
accuracy = calculate_similarity(actual, response)$accuracy
) %>%
ungroup()
calculate_similarity <- function(actual, response, max_dist = 1) {
# Clean and split into words
clean_text <- function(text) {
text %>%
tolower() %>%
str_remove_all("[^a-zA-Z ]") %>%  # Keep letters and spaces
str_squish() %>%
str_split(" +") %>%
unlist()
}
actual_words <- clean_text(actual)
response_words <- clean_text(response)
if(length(actual_words) == 0) return(NA_real_)
# Track matches while avoiding double-counting
matches <- 0
remaining_response <- response_words
for(word in actual_words) {
if(length(remaining_response) == 0) break
distances <- stringdist(word, remaining_response, method = "lv")
min_dist <- min(distances)
if(min_dist <= max_dist) {
matches <- matches + 1
remaining_response <- remaining_response[-which.min(distances)]
}
for(word in actual_words) {
if(length(remaining_response) == 0) break
distances <- stringdist::stringdist(word, remaining_response, method = "lv")
# Handle empty distances and NA values
if(length(distances) == 0 || all(is.na(distances))) next
min_dist <- min(distances, na.rm = TRUE)
# Add validity check for min_dist
if(!is.na(min_dist) && min_dist <= max_dist) {
matches <- matches + 1
remaining_response <- remaining_response[-which.min(distances)]
}
}
}
# Return both strict accuracy and similarity percentage
list(
accuracy = matches / length(actual_words),
similarity = matches / max(length(actual_words), length(response_words))
)
}
# Apply to dataframe
df <- df %>%
mutate(
actual = as.character(actual_sentence),
response = as.character(subject_response)
) %>%
rowwise() %>%
mutate(
accuracy = calculate_similarity(actual, response)$accuracy
) %>%
ungroup()
calculate_similarity <- function(actual, response, max_dist = 1) {
# Clean and split into words
clean_text <- function(text) {
text %>%
tolower() %>%
str_remove_all("[^a-zA-Z ]") %>%  # Keep letters and spaces
str_squish() %>%
str_split(" +") %>%
unlist()
}
actual_words <- clean_text(actual)
response_words <- clean_text(response)
if(length(actual_words) == 0) return(NA_real_)
# Track matches while avoiding double-counting
matches <- 0
remaining_response <- response_words
for(word in actual_words) {
if(length(remaining_response) == 0) break
distances <- stringdist::stringdist(word, remaining_response, method = "lv")
# Handle empty distances and NA values
if(length(distances) == 0 || all(is.na(distances))) next
min_dist <- min(distances, na.rm = TRUE)
# Add validity check for min_dist
if(!is.na(min_dist) && min_dist <= max_dist) {
matches <- matches + 1
remaining_response <- remaining_response[-which.min(distances)]
}
}
}
# Return both strict accuracy and similarity percentage
list(
accuracy = matches / length(actual_words),
similarity = matches / max(length(actual_words), length(response_words))
)
calculate_similarity <- function(actual, response, max_dist = 1) {
# Clean and split into words
clean_text <- function(text) {
text %>%
tolower() %>%
str_remove_all("[^a-zA-Z ]") %>%  # Keep letters and spaces
str_squish() %>%
str_split(" +") %>%
unlist()
}
actual_words <- clean_text(actual)
response_words <- clean_text(response)
if(length(actual_words) == 0) return(NA_real_)
# Track matches while avoiding double-counting
matches <- 0
remaining_response <- response_words
for(word in actual_words) {
if(length(remaining_response) == 0) break
distances <- stringdist::stringdist(word, remaining_response, method = "lv")
# Handle empty distances and NA values
if(length(distances) == 0 || all(is.na(distances))) next
min_dist <- min(distances, na.rm = TRUE)
# Add validity check for min_dist
if(!is.na(min_dist) && min_dist <= max_dist) {
matches <- matches + 1
remaining_response <- remaining_response[-which.min(distances)]
}
}
# Return both strict accuracy and similarity percentage
list(
accuracy = matches / length(actual_words),
similarity = matches / max(length(actual_words), length(response_words))
)
}
# Apply to dataframe
df <- df %>%
mutate(
actual = as.character(actual_sentence),
response = as.character(subject_response)
) %>%
rowwise() %>%
mutate(
accuracy = calculate_similarity(actual, response)$accuracy
) %>%
ungroup()
head(df)
